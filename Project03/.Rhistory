setwd('D:/Heechul/R_Project/Project03')
getwd()
# 패키지 준비
library(dplyr)
library(stringr)
library(ggplot2)
# 데이터 불러오기
data_raw <- read.csv('Data/sales.csv')
head(data_raw)
length(data_raw)
tail(data_raw)
View(data_raw)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data
setwd('D:/Heechul/R_Project/Project03')
getwd()
# 패키지 준비
library(dplyr)
library(stringr)
library(ggplot2)
# 데이터 불러오기
data_raw <- read.csv('Data/sales.csv')
head(data_raw)
length(data_raw)
tail(data_raw)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data <- data[,-c(1,2)]
head(data)
str(data)
# 해당 데이터의 정규성 검토를 함
shapiro.test(data$QTY)
# Linear Regression Model
(fit1 <- lm(QTY~., data = data))
summary(fit1)
# Stepwise regression
# 1. backward elimination
step(fit1, direction = 'backward')
# 2. forward selection
fit2 <- lm(QTY~1, data = data)
step(fit2,
direction = "forward",
scope = ~ITEM_CNT + PRICE + MAXTEMP + SALEDAY + RAIN_DAY + HOLIDAY)
# 3. stepwise regression
step(fit1, direction = 'both')
## HOLIDAY, SALEDAY 변수를 뺀 모델을 fit3로 지정
(fit3 <- lm(QTY ~ ITEM_CNT + PRICE + MAXTEMP + RAIN_DAY, data = data))
summary(fit3)
# 해당 데이터의 정규성 검토를 함
shapiro.test(data$QTY)
data
data
setwd('D:/Heechul/R_Project/Project03')
getwd()
# 패키지 준비
library(dplyr)
library(stringr)
library(ggplot2)
# 데이터 불러오기
data_raw <- read.csv('Data/sales.csv')
head(data_raw)
length(data_raw)
tail(data_raw)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data <- data[,-c(1,2)]
head(data)
str(data)
# 해당 데이터의 정규성 검토를 함
shapiro.test(data$QTY)
# 상과관계 분석
cor(data)
######################################################################################
# 산점도 그려보기
pairs(data)
# Linear Regression Model
(fit1 <- lm(QTY~., data = data))
summary(fit1)
anova(fit1)
# Stepwise regression
# 1. backward elimination
step(fit1, direction = 'backward')
# 2. forward selection
fit2 <- lm(QTY~1, data = data)
step(fit2,
direction = "forward",
scope = ~ITEM_CNT + PRICE + MAXTEMP + SALEDAY + RAIN_DAY + HOLIDAY)
# 3. stepwise regression
step(fit1, direction = 'both')
# 4. All possible regression
library(leaps)
subsets1 <- regsubsets(QTY~., data = data,
method = 'seqrep', nbest = 6)
plot(subsets1)
subsets2 <- regsubsets(QTY~., data = data,
method = 'exhaustive', nbest = 6)
plot(subsets2)
## HOLIDAY, SALEDAY 변수를 뺀 모델을 fit3로 지정
(fit3 <- lm(QTY ~ ITEM_CNT + PRICE + MAXTEMP + RAIN_DAY, data = data))
summary(fit3)
# Checking Assumptions
par(mfrow = c(2, 2))
plot(fit3)
par(mfrow = c(1, 1))
# 1. 정규성 (normality)
qqnorm(fit3$residuals) ; qqline(fit3$residuals)
shapiro.test(fit3$residuals)
# 2. 등분산성 (homoscedasticity)
# 3. 선형성 (linearity)
library(gvlma)
gvmodel <- gvlma(fit3)
summary(gvmodel)
# 4. 독립성 (indepandence)
library(car)
durbinWatsonTest(fit3)
## HOLIDAY, SALEDAY, RAINDAY 변수를 뺀 모델을 fit4로 지정
(fit4 <- lm(QTY ~ ITEM_CNT + PRICE + MAXTEMP, data = data))
summary(fit4)
# Checking Assumptions
par(mfrow = c(2, 2))
plot(fit4)
par(mfrow = c(1, 1))
# 1. 정규성 (normality)
qqnorm(fit4$residuals) ; qqline(fit4$residuals)
shapiro.test(fit4$residuals)
# 2. 등분산성 (homoscedasticity)
# 3. 선형성 (linearity)
library(gvlma)
gvmodel <- gvlma(fit4)
summary(gvmodel)
# 4. 독립성 (indepandence)
library(car)
durbinWatsonTest(fit4)
### 결론
# AIC값은
AIC(fit3, fit4)
# fit4의 다중회귀식을 만들어서 QTY 예측
data['pre_QTY'] = (-1188.2848) + (data['ITEM_CNT'] * (23.0316)) + (data['PRICE'] * (0.7479)) + (data['MAXTEMP'] * (13.6824))
data
data
data
# 데이터 처리
data <- data[,-c(2, 5, 6, 7)]
data
# 데이터 처리
data1 <- data[,-c(2, 5, 6, 7)]
data1
## 모델 성능 평가
library(caret)
## 모델 성능 평가
install.packages('caret')
idx <- createDataPartition(data1$pre_QTY, p = 0.7, list = F)
idx <- createDataPartition(data1$pre_QTY, p = 0.7, list = F)
## 모델 성능 평가
library(caret)
idx <- createDataPartition(data1$pre_QTY, p = 0.7, list = F)
data1_train <- data1[idx, ]     # 70%
data1_test <- data1[-idx, ]     # 30%
table(data1_train$pre_QTY)
table(data1_test$pre_QTY)
install.packages('rpart')
install.packages("rpart")
install.packages('e1071')
install.packages('randomForest')
library(rpart)
library(e1071)
library(randomForest)
data1
head(data1)
rpart.data1 <- rpart(pre_QTY~., data = data1)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1)
rpart.data1 <- rpart(pre_QTY~., data = data1)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
bayes.pred <- predict(bayes.data1, newdata = data1_test, typw = 'class')
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
library(rpart)
library(e1071)
library(randomForest)
rpart.data1 <- rpart(pre_QTY~., data = data1)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
rpart.data1 <- rpart(pre_QTY~., data = data1)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
rpart.data1 <- rpart(pre_QTY~., data = data1)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
bayes.pred <- predict(bayes.data1, newdata = data1_test, type = 'class')
rdf.pred <- predict(rdf.data1, newdata = data1_test, type = 'response')
rpart.data1 <- rpart(pre_QTY~., data = data1_train)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
rpart.data1 <- rpart(pre_QTY~., data = data1_train)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
bayes.pred <- predict(bayes.data1, newdata = data1_test, type = 'class')
rdf.pred <- predict(rdf.data1, newdata = data1_test, type = 'response')
table(data1_test$pre_QTY, rpart.pred)
table(data1_test$pre_QTY, bayes.pred)
data1
data
# 데이터 불러오기
data_raw <- read.csv('Data/sales.csv')
head(data_raw)
length(data_raw)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data <- data[,-c(1,2)]
head(data)
str(data)
# 해당 데이터의 정규성 검토를 함
shapiro.test(data$QTY)
# 상과관계 분석
cor(data)
######################################################################################
# 산점도 그려보기
pairs(data)
# Linear Regression Model
(fit1 <- lm(QTY~., data = data))
summary(fit1)
anova(fit1)
# Stepwise regression
# 1. backward elimination
step(fit1, direction = 'backward')
# 2. forward selection
fit2 <- lm(QTY~1, data = data)
step(fit2,
direction = "forward",
scope = ~ITEM_CNT + PRICE + MAXTEMP + SALEDAY + RAIN_DAY + HOLIDAY)
# 3. stepwise regression
step(fit1, direction = 'both')
# 4. All possible regression
library(leaps)
subsets1 <- regsubsets(QTY~., data = data,
method = 'seqrep', nbest = 6)
plot(subsets1)
subsets2 <- regsubsets(QTY~., data = data,
method = 'exhaustive', nbest = 6)
plot(subsets2)
## HOLIDAY, SALEDAY 변수를 뺀 모델을 fit3로 지정
(fit3 <- lm(QTY ~ ITEM_CNT + PRICE + MAXTEMP + RAIN_DAY, data = data))
summary(fit3)
# Checking Assumptions
par(mfrow = c(2, 2))
plot(fit3)
par(mfrow = c(1, 1))
# 1. 정규성 (normality)
qqnorm(fit3$residuals) ; qqline(fit3$residuals)
shapiro.test(fit3$residuals)
# 2. 등분산성 (homoscedasticity)
# 3. 선형성 (linearity)
library(gvlma)
gvmodel <- gvlma(fit3)
summary(gvmodel)
# 4. 독립성 (indepandence)
library(car)
durbinWatsonTest(fit3)
## HOLIDAY, SALEDAY, RAINDAY 변수를 뺀 모델을 fit4로 지정
(fit4 <- lm(QTY ~ ITEM_CNT + PRICE + MAXTEMP, data = data))
summary(fit4)
# Checking Assumptions
par(mfrow = c(2, 2))
plot(fit4)
par(mfrow = c(1, 1))
# 1. 정규성 (normality)
qqnorm(fit4$residuals) ; qqline(fit4$residuals)
shapiro.test(fit4$residuals)
# 2. 등분산성 (homoscedasticity)
# 3. 선형성 (linearity)
library(gvlma)
gvmodel <- gvlma(fit4)
summary(gvmodel)
# 4. 독립성 (indepandence)
library(car)
durbinWatsonTest(fit4)
### 결론
# AIC값은
AIC(fit3, fit4)
# fit4의 다중회귀식을 만들어서 QTY 예측
data['pre_QTY'] = (-1188.2848) + (data['ITEM_CNT'] * (23.0316)) + (data['PRICE'] * (0.7479)) + (data['MAXTEMP'] * (13.6824))
# fit4의 다중회귀식을 만들어서 QTY 예측
data['pre_QTY'] = (-1188.2848) + (data['ITEM_CNT'] * (23.0316)) + (data['PRICE'] * (0.7479)) + (data['MAXTEMP'] * (13.6824))
data
# 데이터 불러오기
data_raw <- read.csv('Data/sales.csv')
head(data_raw)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data <- data[,-c(1,2)]
head(data)
# 패키지 준비
library(dplyr)
library(stringr)
library(ggplot2)
# 데이터 스포츠,이온음료로 필터링
data <- data_raw %>%
filter(CATEGORY == '비타민음료')
data <- data[,-c(1,2)]
head(data)
# fit4의 다중회귀식을 만들어서 QTY 예측
data['pre_QTY'] = (-1188.2848) + (data['ITEM_CNT'] * (23.0316)) + (data['PRICE'] * (0.7479)) + (data['MAXTEMP'] * (13.6824))
data
# fit4의 다중회귀식을 만들어서 QTY 예측
data['pre_QTY'] = round((-1188.2848) + (data['ITEM_CNT'] * (23.0316)) + (data['PRICE'] * (0.7479)) + (data['MAXTEMP'] * (13.6824)), 0)
data
# 데이터 처리
data1 <- data[,-c(2, 5, 6, 7)]
head(data1)
data1
## 모델 성능 평가
library(caret)
idx <- createDataPartition(data1$pre_QTY, p = 0.7, list = F)
data1_train <- data1[idx, ]     # 70%
data1_test <- data1[-idx, ]     # 30%
table(data1_train$pre_QTY)
table(data1_test$pre_QTY)
library(rpart)
library(e1071)
library(randomForest)
rpart.data1 <- rpart(pre_QTY~., data = data1_train)
bayes.data1 <- naiveBayes(pre_QTY~., data = data1_train)
rdf.data1 <- randomForest(pre_QTY~., data = data1_train, importance = T)
rpart.pred <- predict(rpart.data1, newdata = data1_test, type = 'class')
bayes.pred <- predict(bayes.data1, newdata = data1_test, type = 'class')
rdf.pred <- predict(rdf.data1, newdata = data1_test, type = 'response')
table(data1_test$pre_QTY, rpart.pred)
table(data1_test$pre_QTY, bayes.pred)
bayes.pred
bayes.pred
table(data1_test$pre_QTY, rdf.pred)
confusionMatrix(rdf.pred, data1_test$pre_QTY)
